{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch GVAE\n",
    "基于 PyTorch 实现图变分自编码器 （Graph Variational AutoEncoder，GAE）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入基本包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GCN.__repr__ of GCN (64 -> 32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCN(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0., act=F.relu):\n",
    "        super(GCN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        input = F.dropout(input, self.dropout, self.training)\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "model = GCN(64,32)\n",
    "model.__repr__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Product Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(nn.Module):\n",
    "    \"\"\"Decoder for using inner product for prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout, act=torch.sigmoid):\n",
    "        super(InnerProductDecoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.dropout(z, self.dropout, training=self.training)\n",
    "        adj = self.act(torch.mm(z, z.t()))\n",
    "        return adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GVAE 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVAE(nn.Module):\n",
    "    def __init__(self, input_feat_dim, hidden_dim1, hidden_dim2, dropout):\n",
    "        super(GVAE, self).__init__()\n",
    "        self.gc1 = GCN(input_feat_dim, hidden_dim1, dropout, act=F.relu)\n",
    "        self.gc2 = GCN(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.gc3 = GCN(hidden_dim1, hidden_dim2, dropout, act=lambda x: x)\n",
    "        self.dc = InnerProductDecoder(dropout, act=lambda x: x)\n",
    "\n",
    "    def encode(self, x, adj):\n",
    "        hidden1 = self.gc1(x, adj)\n",
    "        return self.gc2(hidden1, adj), self.gc3(hidden1, adj)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        mu, logvar = self.encode(x, adj)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.dc(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(preds, labels, mu, logvar, n_nodes, norm, pos_weight):\n",
    "    cost = norm * F.binary_cross_entropy_with_logits(preds, labels, pos_weight=pos_weight)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 / n_nodes * torch.mean(torch.sum(\n",
    "        1 + 2 * logvar - mu.pow(2) - logvar.exp().pow(2), 1))\n",
    "    return cost + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "model = 'GVAE'\n",
    "seed = 42\n",
    "epochs = 200\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "lr = 0.01\n",
    "dropout = 0.\n",
    "dataset = 'cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "\n",
    "# utils\n",
    "\n",
    "def load_data(dataset):\n",
    "    # load the data: x, tx, allx, graph\n",
    "    names = ['x', 'tx', 'allx', 'graph']\n",
    "    objects = []\n",
    "    \n",
    "    pbar = trange(len(names))\n",
    "    for i in pbar:\n",
    "        pbar.set_description(\"Load data {}\".format(\"ind.{}.{}\".format(dataset, names[i])))\n",
    "        with open(\"./data/Cora/raw/ind.{}.{}\".format(dataset, names[i]), 'rb') as rf:\n",
    "            data = pkl.load(rf, encoding='latin1')\n",
    "            objects.append(data)\n",
    "        # objects.append(pkl.load(open(\"data/ind.{}.{}\".format(dataset, names[i]), 'rb'), encoding='latin1'))\n",
    "    x, tx, allx, graph = tuple(objects)\n",
    "    \n",
    "    def parse_index_file(filename):\n",
    "        index = []\n",
    "        for line in open(filename, 'rb'):\n",
    "            index.append(int(line.strip()))\n",
    "        return index\n",
    "\n",
    "    test_idx_reorder = parse_index_file(\"./data/Cora/raw/ind.{}.test.index\".format(dataset))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "#     if dataset == 'citeseer':\n",
    "#         # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "#         # Find isolated nodes, add them as zero-vecs into the right position\n",
    "#         test_idx_range_full = range(\n",
    "#             min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "#         tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "#         tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "#         tx = tx_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil() # 转为链表稀疏矩阵lil加快访问速度\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    return adj, features\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 10% positive links\n",
    "    # NOTE: Splits are randomized and results might slightly deviate from reported numbers in the paper.\n",
    "    # TODO: Clean up.\n",
    "\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    assert ~ismember(test_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges_false, edges_all)\n",
    "    assert ~ismember(val_edges, train_edges)\n",
    "    assert ~ismember(test_edges, train_edges)\n",
    "    assert ~ismember(val_edges, test_edges)\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    # NOTE: these edge lists only contain single direction of edge!\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    # return sparse_to_tuple(adj_normalized)\n",
    "    return sparse_mx_to_torch_sparse_tensor(adj_normalized)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "def get_roc_score(emb, adj_orig, edges_pos, edges_neg):\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load data ind.cora.graph: 100%|██████████| 4/4 [00:00<00:00, 570.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708 1433\n",
      "Epoch[10/200], Loss: 0.6952, Val_ap: 0.7553\n",
      "Epoch[20/200], Loss: 0.5651, Val_ap: 0.8352\n",
      "Epoch[30/200], Loss: 0.5069, Val_ap: 0.8874\n",
      "Epoch[40/200], Loss: 0.4816, Val_ap: 0.8912\n",
      "Epoch[50/200], Loss: 0.4664, Val_ap: 0.8934\n",
      "Epoch[60/200], Loss: 0.4591, Val_ap: 0.8981\n",
      "Epoch[70/200], Loss: 0.4526, Val_ap: 0.9012\n",
      "Epoch[80/200], Loss: 0.4474, Val_ap: 0.9069\n",
      "Epoch[90/200], Loss: 0.4427, Val_ap: 0.9121\n",
      "Epoch[100/200], Loss: 0.4401, Val_ap: 0.9110\n",
      "Epoch[110/200], Loss: 0.4385, Val_ap: 0.9112\n",
      "Epoch[120/200], Loss: 0.4369, Val_ap: 0.9116\n",
      "Epoch[130/200], Loss: 0.4355, Val_ap: 0.9099\n",
      "Epoch[140/200], Loss: 0.4338, Val_ap: 0.9052\n",
      "Epoch[150/200], Loss: 0.4322, Val_ap: 0.9062\n",
      "Epoch[160/200], Loss: 0.4309, Val_ap: 0.9064\n",
      "Epoch[170/200], Loss: 0.4301, Val_ap: 0.9059\n",
      "Epoch[180/200], Loss: 0.4288, Val_ap: 0.9047\n",
      "Epoch[190/200], Loss: 0.4277, Val_ap: 0.9048\n",
      "Epoch[200/200], Loss: 0.4269, Val_ap: 0.9035\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9100634071342927\n",
      "Test AP score: 0.9247575722131322\n"
     ]
    }
   ],
   "source": [
    "adj, features = load_data('cora')\n",
    "n_nodes, feat_dim = features.shape\n",
    "print(n_nodes, feat_dim)\n",
    "\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape)\n",
    "adj_orig.eliminate_zeros() # 不为0的元素\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_norm = preprocess_graph(adj)\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "# adj_label = sparse_to_tuple(adj_label)\n",
    "adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "pos_weight = torch.tensor(pos_weight)\n",
    "\n",
    "norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "\n",
    "model = GVAE(feat_dim, hidden1, hidden2, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "hidden_emb = None\n",
    "for epoch in range(epochs):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    recovered, mu, logvar = model(features, adj_norm)\n",
    "    loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                         mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                         norm=norm, pos_weight=pos_weight)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    hidden_emb = mu.data.numpy()\n",
    "    roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Loss: {:.4f}, Val_ap: {:.4f}\" \n",
    "                   .format(epoch+1, epochs, loss.item(), ap_curr))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "print('Test ROC score: ' + str(roc_score))\n",
    "print('Test AP score: ' + str(ap_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
