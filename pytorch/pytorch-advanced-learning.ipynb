{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 编程手册\n",
    "\n",
    "本文代码基于PyTorch 1.0版本，需要用到以下包\n",
    "\n",
    "```python\n",
    "import collections\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torchvision\n",
    "```\n",
    "\n",
    "## 1. 基础配置\n",
    "\n",
    "### (1) check pytorch version\n",
    "\n",
    "```python\n",
    "torch.__version__               # PyTorch version\n",
    "torch.version.cuda              # Corresponding CUDA version\n",
    "torch.backends.cudnn.version()  # Corresponding cuDNN version\n",
    "torch.cuda.get_device_name(0)   # GPU type\n",
    "```\n",
    "\n",
    "### (2) update pytorch\n",
    "\n",
    "```python\n",
    "conda update pytorch torchvision -c pytorch\n",
    "```\n",
    "\n",
    "### (3) random seed setting\n",
    "\n",
    "```python\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "```\n",
    "\n",
    "### (4) 指定程序运行在特定显卡上：\n",
    "\n",
    "在命令行指定环境变量\n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=0,1 python train.py\n",
    "```\n",
    "\n",
    "在代码中指定\n",
    "\n",
    "```python\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "```\n",
    "\n",
    "### (5) 判断是否有CUDA支持\n",
    "\n",
    "```python\n",
    "torch.cuda.is_available()\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "```\n",
    "\n",
    "### (6) 设置为cuDNN benchmark模式\n",
    "\n",
    "Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。\n",
    "\n",
    "```python\n",
    "toch.backends.cudnn.benchmark = True\n",
    "```\n",
    "\n",
    "如果想要避免这种结果波动，设置\n",
    "\n",
    "```python\n",
    "torch.backends.cudnn.deterministic = True\n",
    "```\n",
    "\n",
    "### (7) 手动清除GPU存储\n",
    "\n",
    "有时Control-C中止运行后GPU存储没有及时释放，需要手动清空。在PyTorch内部可以\n",
    "\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程\n",
    "\n",
    "```bash\n",
    " ps aux | grep python    kill -9 [pid]\n",
    "```\n",
    "\n",
    "或者直接重置没有被清空的GPU\n",
    "\n",
    "```bash\n",
    "nvidia-smi --gpu-reset -i [gpu_id]\n",
    "```\n",
    "\n",
    "## 2. 张量处理\n",
    "\n",
    "### (1) 张量的基本信息\n",
    "\n",
    "```python\n",
    "tensor.type()   # Data type\n",
    "tensor.size()\n",
    "# Shape of the tensor. It is a subclass of Python    tuple\n",
    "tensor.dim()    # Number of dimensions.\n",
    "```\n",
    "\n",
    "### (2) 数据类型转换\n",
    "\n",
    "```python\n",
    "# Set default tensor type. Float in PyTorch is much faster than double.\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Type convertions.\n",
    "tensor = tensor.cuda()\n",
    "tensor = tensor.cpu()\n",
    "tensor = tensor.float()\n",
    "tensor = tensor.long()\n",
    "\n",
    "torch.Tensor与np.ndarray转换\n",
    "# torch.Tensor -> np.ndarray.\n",
    "ndarray = tensor.cpu().numpy()\n",
    "\n",
    "# np.ndarray -> torch.Tensor.\n",
    "tensor = torch.from_numpy(ndarray).float()\n",
    "tensor = torch.from_numpy(ndarray.copy()).float()  # If ndarray has negative stride\n",
    "```\n",
    "\n",
    "### (3)  torch.Tensor 与 PIL.Image 转换\n",
    "\n",
    "PyTorch中的张量默认采用N×D×H×W的顺序，并且数据范围在[0, 1]，需要进行转置和规范化。\n",
    "\n",
    "```python\n",
    "# torch.Tensor -> PIL.Image.\n",
    "image = PIL.Image.fromarray(torch.clamp(tensor * 255, min=0, max=255\n",
    "    ).byte().permute(1, 2, 0).cpu().numpy())\n",
    "image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way\n",
    "\n",
    "# PIL.Image -> torch.Tensor.\n",
    "tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))\n",
    "    ).permute(2, 0, 1).float() / 255\n",
    "tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path))  # Equivalently way\n",
    "\n",
    "np.ndarray与PIL.Image转换\n",
    "# np.ndarray -> PIL.Image.\n",
    "image = PIL.Image.fromarray(ndarray.astypde(np.uint8))\n",
    "\n",
    "# PIL.Image -> np.ndarray.\n",
    "ndarray = np.asarray(PIL.Image.open(path))\n",
    "```\n",
    "\n",
    "### (4) 从只包含一个元素的张量中提取值\n",
    "\n",
    "这在训练时统计loss的变化过程中特别有用。否则这将累积计算图，使GPU存储占用量越来越大。\n",
    "\n",
    "```python\n",
    "value = tensor.item()\n",
    "```\n",
    "\n",
    "\n",
    "### (5) 张量形变\n",
    "\n",
    "张量形变: 张量形变常常需要用于将卷积层特征输入全连接层的情形。相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。\n",
    "\n",
    "\n",
    "```python\n",
    "tensor = torch.reshape(tensor, shape)\n",
    "```\n",
    "\n",
    "### (6) 打乱顺序\n",
    "\n",
    "```python\n",
    "    # Shuffle the first dimension\n",
    "    tensor = tensor[torch.randperm(tensor.size(0))]\n",
    "```\n",
    "\n",
    "### (7) 复制张量: 有三种复制的方式，对应不同的需求。\n",
    "\n",
    "| Operation             | New/Shared memory | Still in computation graph |\n",
    "| --------------------- | ----------------- | -------------------------- |\n",
    "| tensor.clone()        | New               | Yes                        |\n",
    "| tensor.detach()       | Shared            | No                         |\n",
    "| tensor.detach.clone() | New               | No                         |\n",
    "\n",
    "### (8) 拼接张量\n",
    "\n",
    "注意`torch.cat`和`torch.stack`的区别在于`torch.cat`沿着给定的维度拼接，而`torch.stack`会新增一维。例如当参数是3个10×5的张量，`torch.cat`的结果是30×5的张量，而`torch.stack`的结果是3×10×5的张量。\n",
    "\n",
    "```python\n",
    "tensor = torch.cat(list_of_tensors, dim=0)\n",
    "tensor = torch.stack(list_of_tensors, dim=0)\n",
    "```\n",
    "\n",
    "### (9) 将整数标记转换成独热（one-hot）编码\n",
    " (PyTorch中的标记默认从0开始)\n",
    "\n",
    "```python\n",
    "   N = tensor.size(0)\n",
    "   one_hot = torch.zeros(N, num_classes).long()\n",
    "   one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())\n",
    "```\n",
    "\n",
    "### (10)得到非零/零元素\n",
    "\n",
    "```python\n",
    "   torch.nonzero(tensor)               # Index of non-zero elements\n",
    "   torch.nonzero(tensor == 0)          # Index of zero elements\n",
    "   torch.nonzero(tensor).size(0)       # Number of non-zero elements\n",
    "   torch.nonzero(tensor == 0).size(0)  # Number of zero elements\n",
    "```\n",
    "\n",
    "### (11)张量扩展\n",
    "\n",
    "```python\n",
    "   # Expand tensor of shape 64*512 to shape 64*512*7*7.\n",
    "   torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)\n",
    "```\n",
    "\n",
    "### (12)矩阵乘法\n",
    "\n",
    "```python\n",
    "# Matrix multiplication: (m*n) * (n*p) -> (m*p).\n",
    "result = torch.mm(tensor1, tensor2)\n",
    "\n",
    "# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p).\n",
    "result = torch.bmm(tensor1, tensor2)\n",
    "\n",
    "# Element-wise multiplication.\n",
    "result = tensor1 * tensor2\n",
    "```\n",
    "\n",
    "### (13) 计算两组数据之间的两两欧式距离\n",
    "\n",
    "```\n",
    "# X1 is of shape m*d.\n",
    "X1 = torch.unsqueeze(X1, dim=1).expand(m, n, d)\n",
    "# X2 is of shape n*d.\n",
    "X2 = torch.unsqueeze(X2, dim=0).expand(m, n, d)\n",
    "# dist is of shape m*n, where dist[i][j] = sqrt(|X1[i, :] - X[j, :]|^2)\n",
    "dist = torch.sqrt(torch.sum((X1 - X2) ** 2, dim=2))\n",
    "```\n",
    "\n",
    "### 3. 模型定义\n",
    "\n",
    "### (1) 卷积层\n",
    "\n",
    "最常用的卷积层配置是：\n",
    "\n",
    "```python\n",
    "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "```\n",
    "\n",
    "如果卷积层配置比较复杂，不方便计算输出大小时，可以利用如下可视化工具辅助: <https://ezyang.github.io/convolution-visualizer/index.html>\n",
    "\n",
    "### (2) GAP（Global average pooling）层\n",
    "```\n",
    "   gap = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
    "```\n",
    "\n",
    "### (3) 多卡同步BN（Batch normalization）\n",
    "\n",
    "当使用torch.nn.DataParallel将代码运行在多张GPU卡上时，PyTorch的BN层默认操作是各卡上数据独立地计算均值和标准差，同步BN使用所有卡上的数据一起计算BN层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。\n",
    "\n",
    "参见： [Synchronized-BatchNorm-PyTorch​github](vacancy/Synchronized-BatchNorm-PyTorch​github.com)\n",
    "\n",
    "### (4) 计算模型参数量[D]\n",
    "\n",
    "```\n",
    "# Total parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "# Trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "\n",
    "类似Keras的model.summary()输出模型信息，参见[pytorch-summary​github](sksq96/pytorch-summary​github.com)\n",
    "\n",
    "### (5) 模型权值初始化[D]\n",
    "\n",
    "注意`model.modules()`和`model.children()`的区别：`model.modules()`会迭代地遍历模型的所有子层，而`model.children()`只会遍历模型下的一层。\n",
    "\n",
    "```python\n",
    "# Common practise for initialization.\n",
    "for m in model.modules():\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_out',\n",
    "                                      nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, val=0.0)\n",
    "\n",
    "    elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1.0)\n",
    "        torch.nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "# Initialization with given tensor.\n",
    "m.weight = torch.nn.Parameter(tensor)\n",
    "```\n",
    "\n",
    "### (6) 部分层使用预训练模型\n",
    "\n",
    "注意如果保存的模型是`torch.nn.DataParallel`，则当前的模型也需要是`torch.nn.DataParallel`。`torch.nn.DataParallel(model).module == model`。\n",
    "\n",
    "```python\n",
    "   model.load_state_dict(torch.load('model,pth'), strict=False)\n",
    "```\n",
    "\n",
    "将在GPU保存的模型加载到CPU:\n",
    "\n",
    "```python\n",
    "   model.load_state_dict(torch.load('model,pth', map_location='cpu'))\n",
    "```\n",
    "\n",
    "### 4. 特征提取与微调\n",
    "\n",
    "### (1) 提取ImageNet预训练模型某层的卷积特征\n",
    "\n",
    "```python\n",
    "# VGG-16 relu5-3 feature.\n",
    "model = torchvision.models.vgg16(pretrained=True).features\n",
    "# VGG-16 pool5 feature.\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model = torch.nn.Sequential(model.features, model.avgpool)\n",
    "# VGG-16 fc7 feature.\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])\n",
    "# ResNet GAP feature.\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model = torch.nn.Sequential(collections.OrderedDict(\n",
    "    list(model.named_children())[:-1]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    conv_representation = model(image)\n",
    "```\n",
    "\n",
    "### (2) 提取ImageNet预训练模型多层的卷积特征\n",
    "\n",
    "```python\n",
    "class FeatureExtractor(torch.nn.Module):\n",
    "    \"\"\"Helper class to extract several convolution features from the given\n",
    "    pre-trained model.\n",
    "\n",
    "    Attributes:\n",
    "        _model, torch.nn.Module.\n",
    "        _layers_to_extract, list<str> or set<str>\n",
    "\n",
    "    Example:\n",
    "        >>> model = torchvision.models.resnet152(pretrained=True)\n",
    "        >>> model = torch.nn.Sequential(collections.OrderedDict(\n",
    "                list(model.named_children())[:-1]))\n",
    "        >>> conv_representation = FeatureExtractor(\n",
    "                pretrained_model=model,\n",
    "                layers_to_extract={'layer1', 'layer2', 'layer3', 'layer4'})(image)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model, layers_to_extract):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self._model = pretrained_model\n",
    "        self._model.eval()\n",
    "        self._layers_to_extract = set(layers_to_extract)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            conv_representation = []\n",
    "            for name, layer in self._model.named_children():\n",
    "                x = layer(x)\n",
    "                if name in self._layers_to_extract:\n",
    "                    conv_representation.append(x)\n",
    "            return conv_representation\n",
    "```\n",
    "\n",
    "### (3)其他预训练模型\n",
    "[pretrained-models](Cadene/pretrained-models.pytorchgithub.com)\n",
    "\n",
    "### (4) 微调全连接层\n",
    "\n",
    "```python\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc = nn.Linear(512, 100)  # Replace the last fc layer\n",
    "optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "以较大学习率微调全连接层，较小学习率微调卷积层\n",
    "\n",
    "```python\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "finetuned_parameters = list(map(id, model.fc.parameters()))\n",
    "conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)\n",
    "parameters = [{'parameters': conv_parameters, 'lr': 1e-3},\n",
    "              {'parameters': model.fc.parameters()}]\n",
    "optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "```\n",
    "\n",
    "## 5. 模型训练\n",
    "\n",
    "### (1) 常见训练和验证数据预处理\n",
    "\n",
    "ToTensor操作会将PIL.Image或形状为H×W×D，数值范围为[0, 255]的np.ndarray转换为形状为D×H×W，数值范围为[0.0, 1.0]的torch.Tensor。\n",
    "\n",
    "```python\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=224,\n",
    "                                             scale=(0.08, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    " ])\n",
    " val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                     std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "```\n",
    "\n",
    "### (2) 训练基本代码框架\n",
    "\n",
    "```python\n",
    "for t in epoch(80):\n",
    "    for images, labels in tqdm.tqdm(train_loader, desc='Epoch %3d' % (t + 1)):\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        scores = model(images)\n",
    "        loss = loss_function(scores, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "### (3)  label smothing\n",
    "\n",
    "```python\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    N = labels.size(0)\n",
    "    # C is the number of classes.\n",
    "    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)\n",
    "\n",
    "    score = model(images)\n",
    "    log_prob = torch.nn.functional.log_softmax(score, dim=1)\n",
    "    loss = -torch.sum(log_prob * smoothed_labels) / N\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### (4) Mixup\n",
    "\n",
    "```python\n",
    "beta_distribution = torch.distributions.beta.Beta(alpha, alpha)\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    # Mixup images.\n",
    "    lambda_ = beta_distribution.sample([]).item()\n",
    "    index = torch.randperm(images.size(0)).cuda()\n",
    "    mixed_images = lambda_ * images + (1 - lambda_) * images[index, :]\n",
    "\n",
    "    # Mixup loss.\n",
    "    scores = model(mixed_images)\n",
    "    loss = (lambda_ * loss_function(scores, labels)\n",
    "            + (1 - lambda_) * loss_function(scores, labels[index]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### (5) 双线性汇合（bilinear pooling）\n",
    "\n",
    "```python\n",
    "X = torch.reshape(N, D, H * W)                        # Assume X has shape N*D*H*W\n",
    "X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling\n",
    "assert X.size() == (N, D, D)\n",
    "X = torch.reshape(X, (N, D * D))\n",
    "X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization\n",
    "X = torch.nn.functional.normalize(X)                  # L2 normalization\n",
    "```\n",
    "\n",
    "### (6) L1 正则化\n",
    "\n",
    "```python\n",
    "l1_regularization = torch.nn.L1Loss(reduction='sum')\n",
    "loss = ...  # Standard cross-entropy loss\n",
    "for param in model.parameters():\n",
    "    loss += torch.sum(torch.abs(param))\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "reg = 1e-6\n",
    "l2_loss = Variable(torch.FloatTensor(1), requires_grad=True)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        l2_loss = l2_loss + (0.5 * reg * torch.sum(torch.pow(W, 2)))\n",
    "```\n",
    "\n",
    "### (7) 不对偏置项进行L2正则化/权值衰减（weight decay）\n",
    "\n",
    "```python\n",
    "bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')\n",
    "others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')\n",
    "parameters = [{'parameters': bias_list, 'weight_decay': 0},\n",
    "              {'parameters': others_list}]\n",
    "optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "```\n",
    "\n",
    "### (8) 梯度裁剪（gradient clipping）\n",
    "\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)\n",
    "```\n",
    "\n",
    "\n",
    "### (9) 计算Softmax 输出的正确率\n",
    "\n",
    "```python\n",
    "score = model(images)\n",
    "prediction = torch.argmax(score, dim=1)\n",
    "num_correct = torch.sum(prediction == labels).item()\n",
    "accuruacy = num_correct / labels.size(0)\n",
    "```\n",
    "\n",
    "### (10) 可视化模型前馈计算图：\n",
    "\n",
    "https://github.com/szagoruyko/pytorchviz\n",
    "\n",
    "### （11）可视化学习曲线\n",
    "\n",
    "有Facebook自己开发的Visdom和Tensorboard两个选择。\n",
    "facebookresearch/visdomgithub.com\n",
    "lanpa/tensorboardXgithub.com\n",
    "\n",
    "```python\n",
    "# Example using Visdom.\n",
    "vis = visdom.Visdom(env='Learning curve', use_incoming_socket=False)\n",
    "assert self._visdom.check_connection()\n",
    "self._visdom.close()\n",
    "options = collections.namedtuple('Options', ['loss', 'acc', 'lr'])(\n",
    "    loss={'xlabel': 'Epoch', 'ylabel': 'Loss', 'showlegend': True},\n",
    "    acc={'xlabel': 'Epoch', 'ylabel': 'Accuracy', 'showlegend': True},\n",
    "    lr={'xlabel': 'Epoch', 'ylabel': 'Learning rate', 'showlegend': True})\n",
    "\n",
    "for t in epoch(80):\n",
    "    tran(...)\n",
    "    val(...)\n",
    "    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_loss]),\n",
    "             name='train', win='Loss', update='append', opts=options.loss)\n",
    "    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_loss]),\n",
    "             name='val', win='Loss', update='append', opts=options.loss)\n",
    "    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([train_acc]),\n",
    "             name='train', win='Accuracy', update='append', opts=options.acc)\n",
    "    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([val_acc]),\n",
    "             name='val', win='Accuracy', update='append', opts=options.acc)\n",
    "    vis.line(X=torch.Tensor([t + 1]), Y=torch.Tensor([lr]),\n",
    "             win='Learning rate', update='append', opts=options.lr)\n",
    "```\n",
    "\n",
    "### （12）得到当前学习率\n",
    "\n",
    "If there is one global learning rate (which is the common case):\n",
    "```python\n",
    "lr = next(iter(optimizer.param_groups))['lr']\n",
    "```\n",
    "\n",
    "If there are multiple learning rates for different layers.\n",
    "``` python\n",
    "all_lr = []\n",
    "for param_group in optimizer.param_groups:\n",
    "    all_lr.append(param_group['lr'])\n",
    "```\n",
    "\n",
    "### （13）学习率衰减\n",
    "\n",
    "```python\n",
    "# Reduce learning rate when validation accuarcy plateau.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "for t in range(0, 80):\n",
    "    train(...); val(...)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "# Cosine annealing learning rate.\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)\n",
    "# Reduce learning rate by 10 at given epochs.\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)\n",
    "for t in range(0, 80):\n",
    "    scheduler.step()\n",
    "    train(...); val(...)\n",
    "\n",
    "# Learning rate warmup by 10 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)\n",
    "for t in range(0, 10):\n",
    "    scheduler.step()\n",
    "    train(...); val(...)\n",
    "```\n",
    "\n",
    "##### （14）保存与加载断点\n",
    "\n",
    "注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。\n",
    "\n",
    "```python\n",
    "# Save checkpoint.\n",
    "is_best = current_acc > best_acc\n",
    "best_acc = max(best_acc, current_acc)\n",
    "checkpoint = {\n",
    "    'best_acc': best_acc,\n",
    "    'epoch': t + 1,\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "model_path = os.path.join('model', 'checkpoint.pth.tar')\n",
    "torch.save(checkpoint, model_path)\n",
    "if is_best:\n",
    "    shutil.copy('checkpoint.pth.tar', model_path)\n",
    "\n",
    "# Load checkpoint.\n",
    "if resume:\n",
    "    model_path = os.path.join('model', 'checkpoint.pth.tar')\n",
    "    assert os.path.isfile(model_path)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print('Load checkpoint at epoch %d.' % start_epoch)\n",
    "```\n",
    "\n",
    "\n",
    "#### 6. Pytorch 其他注意事项\n",
    "\n",
    "### (1) 模型定义\n",
    "\n",
    "- 建议有参数的层和汇合（pooling）层使用`torch.nn`模块定义，激活函数直接使用 `torch.nn.functional`。`torch.nn`模块和`torch.nn.functional`的区别在于，`torch.nn`模块在计算时底层调用了`torch.nn.functional`，但`torch.nn`模块包括该层参数，还可以应对训练和测试两种网络状态。使用`torch.nn.functional`时要注意网络状态，如\n",
    "\n",
    "```\n",
    "def forward(self, x):\n",
    "    ...\n",
    "    x = torch.nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "```\n",
    "\n",
    "- `model(x)`前用 `model.train()`和 `model.eval()`切换网络状态。不需要计算梯度的代码块用 `with torch.no_grad()`包含起来。`model.eval()`和`torch.no_grad()`的区别在于，`model.eval()`是将网络切换为测试状态，例如BN和随机失活（dropout）在训练和测试阶段使用不同的计算方法。`torch.no_grad()`是关闭PyTorch张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行`loss.backward()`。`torch.nn.CrossEntropyLoss`的输入不需要经过`Softmax`。\n",
    "\n",
    "- `torch.nn.CrossEntropyLoss`等价于`torch.nn.functional.log_softmax` + `torch.nn.NLLLoss`。\n",
    "\n",
    "- `loss.backward()`前用`optimizer.zero_grad()`清除累积梯度。\n",
    "\n",
    "- `optimizer.zero_grad()`和`model.zero_grad()`效果一样。\n",
    "\n",
    "### (2) PyTorch性能与调试\n",
    "\n",
    "- `torch.utils.data.DataLoader`中尽量设置`pin_memory=True`，对特别小的数据集如MNIST设置`pin_memory=False` 反而更快一些。\n",
    "- `num_workers` 的设置需要在实验中找到最快的取值。\n",
    "- 用`del`及时删除不用的中间变量，节约GPU存储。\n",
    "- 使用`inplace`操作可节约 GPU 存储，如\n",
    "\n",
    "```python\n",
    "x = torch.nn.functional.relu(x, inplace=True)\n",
    "```\n",
    "\n",
    "- 减少CPU和GPU之间的数据传输。例如， 如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。\n",
    "- 使用半精度浮点数`half()`会有一定的速度提升，具体效率依赖于GPU型号。需要小心数值精度过低带来的稳定性问题。时常使用 `assert tensor.size() == (N, D, H, W)`作为调试手段，确保张量维度和你设想中一致。\n",
    "- 除了标记 y 外，尽量少使用一维张量，使用n*1的二维张量代替，可以避免一些意想不到的一维张量计算结果。\n",
    "- 统计代码各部分耗时\n",
    "\n",
    "```python\n",
    "with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:\n",
    "    ...\n",
    "    print(profile)\n",
    "```\n",
    "\n",
    "或者在命令行运行：\n",
    "\n",
    "```bash\n",
    "python -m torch.utils.bottleneck main.py\n",
    "```\n",
    "\n",
    "参考链接：\n",
    "\n",
    "- Tensorflow cookbook\n",
    "\n",
    "- https://github.com/kevinzakka/pytorch-goodies\n",
    "\n",
    "- https://github.com/chenyuntc/pytorch-book\n",
    "\n",
    "- pytorch 官方文档和tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
