{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个张量 `tensor` 可以存储一个标量数值、一个数组、一个矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量\n",
    "scalar = torch.tensor(3)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数组\n",
    "array = torch.tensor([1, 2])\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵\n",
    "matrix = torch.zeros([2, 2])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1750, 0.1227],\n",
       "         [0.4404, 0.5745]],\n",
       "\n",
       "        [[0.5307, 0.0565],\n",
       "         [0.1978, 0.2537]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 任意维度张量\n",
    "multi_tensor = torch.rand([2, 2, 2])\n",
    "multi_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量可以高效的执行代数的运算。机器学习应用中最常见的运算就是矩阵乘法。例如将两个随机矩阵进行相乘，维度分别是 3x\n",
    "5 和 5x4 ，这个运算可以通过矩阵相乘运算 `@` 实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2507, -1.7795,  1.7400, -0.6592],\n",
       "        [ 1.5113,  0.0756,  0.7788, -0.4260],\n",
       "        [-0.9691, -4.6163,  0.6580, -1.1635]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([3, 5])\n",
    "y = torch.randn([5, 4])\n",
    "z = x @ y\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过 `x+y` 运算将两个 tensor 相加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 tensor 转为 numpy array 可以调用 `.numpy()` 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25068876, -1.7794524 ,  1.7400146 , -0.65915424],\n",
       "       [ 1.5113411 ,  0.07561732,  0.77881896, -0.4260341 ],\n",
       "       [-0.96910274, -4.616257  ,  0.65803266, -1.1635164 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 numpy array 转为 tensor 可以调用 `torch.tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8194, 4.7365], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = torch.tensor(np.random.normal([3, 5]))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 自动微分\n",
    "PyTorch 中相比 numpy 最大优点就是可以实现自动微分，对于优化神经网络参数的应用非常有帮助。本文通过举例来理解微分的过程。\n",
    "\n",
    "假设复合函数 $g(u(x))$，可以通过链式法则计算 $g$ 对 $x$ 的导数：\n",
    "$$\\frac{d g}{d x}=\\frac{d g}{d u} * \\frac{d u}{d x}$$\n",
    "\n",
    "以下说明 PyTorch 如何实现自动求导的过程。\n",
    "\n",
    "为了在 PyTorch 中计算导数，首先要创建一个张量并设置其 `requires_grad = True`，然后利用张量运算来定义函数，假设 $u$ 是一个二次方的函数，而 `g` 是一个简单的线性函数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "def u(x):\n",
    "  return x * x\n",
    "\n",
    "def g(u):\n",
    "  return -u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例中复合函数是 $g(u(x))=-x^2$，所以导数是 $-2x$，如果 $x=1$ ，那么导数值为 $-2$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中调用梯度函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgdx = torch.autograd.grad(g(u(x)), x)[0]\n",
    "dgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拟合曲线\n",
    "为了展示自动微分的作用，此处介绍另一个例子。\n",
    "\n",
    "首先假设有一些服从一个曲线（也就是函数 $f(x)=5x^2+3$）的样本，然后希望基于这些样本来评估这个函数 f(x) 。首先定义一个带参数的函数:\n",
    "$$g(x, w)=w_{0} x^{2}+w_{1} x+w_{2}$$\n",
    "\n",
    "函数的输入是 $x$，然后 $w$ 是参数，目标是找到合适的参数使得下列式子成立：\n",
    "$$g(x, w)=f(x)$$\n",
    "\n",
    "实现的一个方法可以是通过优化下面的损失函数来实现:\n",
    "$$L(w)=\\sum(f(x)-g(x, w))^{2}$$\n",
    "\n",
    "此处定义的这个问题里有一个已知的函数即 $f(x)$ ，但可以采用一个更加通用的方法，可以应用到任何一个可微分的函数，并采用**随机梯度下降法**求解参数，即通过计算 $L(w)$ 对于每个参数 $w$ 的梯度的平均值。\n",
    "\n",
    "在PyTorch实现过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevTools\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4.9990711e+00],\n",
       "       [-1.3374003e-04],\n",
       "       [ 3.0566640e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming we know that the desired function is a polynomial of 2nd degree, we\n",
    "# allocate a vector of size 3 to hold the coefficients and initialize it with\n",
    "# random noise.\n",
    "w = torch.tensor(torch.randn([3, 1]), requires_grad=True)\n",
    "\n",
    "# We use the Adam optimizer with learning rate set to 0.1 to minimize the loss.\n",
    "opt = torch.optim.Adam([w], 0.1)\n",
    "\n",
    "def model(x):\n",
    "    # We define yhat to be our estimate of y.\n",
    "    f = torch.stack([x * x, x, torch.ones_like(x)], 1)\n",
    "    yhat = torch.squeeze(f @ w, 1)\n",
    "    return yhat\n",
    "\n",
    "def compute_loss(y, yhat):\n",
    "    # The loss is defined to be the mean squared error distance between our\n",
    "    # estimate of y and its true value. \n",
    "    loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "    return loss\n",
    "\n",
    "def generate_data():\n",
    "    # Generate some training data based on the true function\n",
    "    x = torch.rand(100) * 20 - 10\n",
    "    y = 5 * x * x + 3\n",
    "    return x, y\n",
    "\n",
    "def train_step():\n",
    "    x, y = generate_data()\n",
    "\n",
    "    yhat = model(x)\n",
    "    loss = compute_loss(y, yhat)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "for _ in range(1000):\n",
    "    train_step()\n",
    "\n",
    "w.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可知与定义函数的参数非常接近，以上仅是简单的函数PyTorch可以拟合更复杂的函数。很多问题，比如优化一个带有上百万参数的神经网络，都可以用 PyTorch 高效实现，PyTorch 可以跨多个设备和线程进行拓展，并且支持多个平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 模型封装\n",
    "在前面的示例中，我们直接使用了张量和张量操作来构建模型。为了使代码更有条理，建议使用PyTorch的模块。模块只是参数的容器，并且封装了模型操作。例如表示线性模型$y = ax + b$。该模型可以用以下代码表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.a = torch.nn.Parameter(torch.rand(1))\n",
    "    self.b = torch.nn.Parameter(torch.rand(1))\n",
    "\n",
    "  def forward(self, x):\n",
    "    yhat = self.a * x + self.b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用的例子如下所示，需要实例化声明的模型，并且像调用函数一样使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4563, 1.2731, 2.0899, 2.9067, 3.7235, 4.5403, 5.3571, 6.1739, 6.9907,\n",
       "        7.8075], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10, dtype=torch.float32)\n",
    "\n",
    "net = Net()\n",
    "y = net(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数都是设置 `requires_grad` 为 `true` 的张量。通过模型的 `parameters()` 方法可以很方便的访问和使用参数，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.8168], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4563], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设是一个未知的函数 $y=5x+3+n$ ，注意这里的 $n$ 是表示噪音，然后希望优化模型参数来拟合这个函数，首先可以简单从这个函数进行采样，得到一些样本数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n",
       "         0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1600, 0.1700,\n",
       "         0.1800, 0.1900, 0.2000, 0.2100, 0.2200, 0.2300, 0.2400, 0.2500, 0.2600,\n",
       "         0.2700, 0.2800, 0.2900, 0.3000, 0.3100, 0.3200, 0.3300, 0.3400, 0.3500,\n",
       "         0.3600, 0.3700, 0.3800, 0.3900, 0.4000, 0.4100, 0.4200, 0.4300, 0.4400,\n",
       "         0.4500, 0.4600, 0.4700, 0.4800, 0.4900, 0.5000, 0.5100, 0.5200, 0.5300,\n",
       "         0.5400, 0.5500, 0.5600, 0.5700, 0.5800, 0.5900, 0.6000, 0.6100, 0.6200,\n",
       "         0.6300, 0.6400, 0.6500, 0.6600, 0.6700, 0.6800, 0.6900, 0.7000, 0.7100,\n",
       "         0.7200, 0.7300, 0.7400, 0.7500, 0.7600, 0.7700, 0.7800, 0.7900, 0.8000,\n",
       "         0.8100, 0.8200, 0.8300, 0.8400, 0.8500, 0.8600, 0.8700, 0.8800, 0.8900,\n",
       "         0.9000, 0.9100, 0.9200, 0.9300, 0.9400, 0.9500, 0.9600, 0.9700, 0.9800,\n",
       "         0.9900]),\n",
       " tensor([3.2779, 3.2087, 3.1962, 3.3549, 3.4230, 3.2562, 3.5627, 3.5974, 3.5186,\n",
       "         3.5222, 3.5088, 3.8233, 3.7422, 3.9081, 3.9633, 3.9187, 3.8353, 3.9797,\n",
       "         3.9560, 4.0946, 4.2978, 4.1579, 4.3780, 4.2639, 4.2618, 4.5238, 4.5502,\n",
       "         4.5926, 4.5279, 4.7486, 4.5882, 4.7504, 4.8934, 4.8166, 4.8886, 4.9763,\n",
       "         4.8860, 5.0221, 5.0001, 4.9692, 5.0333, 5.2196, 5.1132, 5.4034, 5.2557,\n",
       "         5.3655, 5.5712, 5.4512, 5.6056, 5.5024, 5.6431, 5.6173, 5.7256, 5.8174,\n",
       "         5.8231, 5.9153, 6.0833, 6.0174, 5.9477, 6.0034, 6.2931, 6.1538, 6.2601,\n",
       "         6.2086, 6.2600, 6.3072, 6.3509, 6.4330, 6.6434, 6.6078, 6.7903, 6.5616,\n",
       "         6.6434, 6.8000, 6.7974, 6.7805, 6.8924, 7.0068, 6.9495, 7.0473, 7.0343,\n",
       "         7.2268, 7.3588, 7.2624, 7.2681, 7.5089, 7.4445, 7.4037, 7.6953, 7.7057,\n",
       "         7.6892, 7.6695, 7.6207, 7.8525, 7.8618, 7.9645, 7.8519, 8.0322, 7.9964,\n",
       "         8.1022]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(100, dtype=torch.float32) / 100\n",
    "y = 5 * x + 3 + torch.rand(100) * 0.3\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和上一个例子类似，需要定义一个损失函数并优化模型的参数，如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([4.9549], requires_grad=True) Parameter containing:\n",
      "tensor([3.1692], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(10000):\n",
    "  net.zero_grad()\n",
    "  yhat = net(x)\n",
    "  loss = criterion(yhat, y)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(net.a, net.b) # Should be close to 5 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中已经实现了很多预定义好的模块。比如 `torch.nn.Linear` 就是一个类似上述例子中定义的一个更加通用的线性函数，所以我们可以采用这个函数来重写模型代码，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear = torch.nn.Linear(1, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    yhat = self.linear(x.unsqueeze(1)).squeeze(1)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处使用两个函数，`squeeze` 和 `unsqueeze` ，主要是 `torch.nn.Linear` 会对一批向量而不是数值进行操作。\n",
    "\n",
    "默认调用 `parameters()` 会返回其所有子模块的参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.9434]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9605], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "for p in net.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以预定义模块作为包容其他模块的容器，最常用的就是 `torch.nn.Sequential` ，它的名字就暗示了它主要用于堆叠多个模块（或者网络层），例如堆叠两个线性网络层，中间是一个非线性函数 `ReLU` ，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(64, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, 10),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 广播机制的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点\n",
    "PyTorch 支持广播的元素积运算。正常情况下，当想执行类似加法和乘法操作的时候，你需要确认操作数的形状是匹配的，比如无法进行一个 [3, 2] 大小的张量和 [3, 4] 大小的张量的加法操作。\n",
    "\n",
    "但是存在一种特殊的情况：只有单一维度的时候，PyTorch 会隐式的根据另一个操作数的维度来拓展只有单一维度的操作数张量。因此，实现 [3,2] 大小的张量和 [3,1] 大小的张量相加的操作是合法的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[1.], [2.]])\n",
    "# c = a + b.repeat([1, 2])\n",
    "# 不同维度可以直接相加\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播机制可以实现**隐式**的维度复制操作（`repeat` 操作），并且代码更短，内存使用上也更加高效，因为不需要存储复制的数据的结果。这个机制非常适合用于结合多个维度不同的特征的时候。\n",
    "\n",
    "为了拼接不同维度的特征，通常的做法是先对输入张量进行维度上的复制，然后拼接后使用非线性激活函数。整个过程的代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([5, 3, 5])\n",
    "b = torch.rand([5, 1, 6])\n",
    "\n",
    "linear = torch.nn.Linear(11, 10)\n",
    "\n",
    "# concat a and b and apply nonlinearity\n",
    "tiled_b = b.repeat([1, 3, 1]) # b shape:  [5, 3, 6]\n",
    "c = torch.cat([a, tiled_b], 2) # c shape: [5, 3, 11]\n",
    "d = torch.nn.functional.relu(linear(c))\n",
    "\n",
    "print(d.shape)  # torch.Size([5, 3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但实际上通过广播机制可以实现得更加高效，即 $f(m(x+y))$ 是等同于 $f(mx+my)$ 的，也就是我们可以先分别做线性操作，然后通过广播机制来做隐式的拼接操作，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([5, 3, 5])\n",
    "b = torch.rand([5, 1, 6])\n",
    "\n",
    "linear1 = torch.nn.Linear(5, 10)\n",
    "linear2 = torch.nn.Linear(6, 10)\n",
    "\n",
    "pa = linear1(a) # pa shape: [5, 3, 10]\n",
    "pb = linear2(b) # pb shape: [5, 1, 10]\n",
    "d = torch.nn.functional.relu(pa + pb)\n",
    "\n",
    "print(d.shape)  # torch.Size([5, 3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上这段代码非常通用，可以用于任意维度大小的张量，只要它们之间是可以实现广播机制的，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Merge(torch.nn.Module):\n",
    "    def __init__(self, in_features1, in_features2, out_features, activation=None):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(in_features1, out_features)\n",
    "        self.linear2 = torch.nn.Linear(in_features2, out_features)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        pa = self.linear1(a)\n",
    "        pb = self.linear2(b)\n",
    "        c = pa + pb\n",
    "        if self.activation is not None:\n",
    "            c = self.activation(c)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺点\n",
    "上述内容是广播机制的优点。但它的缺点是什么呢？原因也是出现在隐式的操作，这种做法非常不利于进行代码的调试。\n",
    "以例子说明："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.], [2.]])\n",
    "b = torch.tensor([1., 2.])\n",
    "c = torch.sum(a + b)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以上述代码的输出结果 c 是什么呢？你可能觉得是 6，但这是错的，正确答案是 12 。这是因为当两个张量的维度不匹配的时候，PyTorch 会自动将维度低的张量的第一个维度进行拓展，然后在进行元素之间的运算，所以这里会将b 先拓展为 [[1, 2], [1, 2]]，然后 a+b 的结果应该是 [[2,3], [3, 4]] ，然后sum 操作是将所有元素求和得到结果 12。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么避免这种结果的方法就是**显式**的操作，比如在这个例子中就需要指定好想要求和的维度，这样进行代码调试会更简单，代码修改后如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.], [2.]])\n",
    "b = torch.tensor([1., 2.])\n",
    "c = torch.sum(a + b, 0)\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里得到的 c 的结果是 [5, 7]，而我们基于结果的维度可以知道出现了错误。\n",
    "\n",
    "这有个通用的做法，就是在做累加（ reduction ）操作或者使用 `torch.squeeze` 的时候总是指定好维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 重载运算符的使用\n",
    "与 NumPy 类似，PyTorch 会重载 python 的一些**运算符**来让 PyTorch 代码更简短和更有可读性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如，切片操作就是其中一个重载的运算符，可以更容易的对张量进行索引操作，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.5239,  1.0537,  0.6141, -0.1963, -0.9043]),\n",
       " tensor([1.0537, 0.6141]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(5)\n",
    "begin = 1\n",
    "end = 3\n",
    "z = x[begin:end]\n",
    "x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0537, 0.6141])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 等同于narrow\n",
    "z = torch.narrow(x, 0, begin, end-begin)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但需要谨慎使用这个运算符，过度使用可以导致代码变得低效。以下示例说明该运算符如何使代码低效！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.004023 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 矩阵行累加\n",
    "import torch\n",
    "import time\n",
    "\n",
    "x = torch.rand([500, 10])\n",
    "\n",
    "z = torch.zeros([10])\n",
    "\n",
    "start = time.time()\n",
    "for i in range(500):\n",
    "    z += x[i]\n",
    "print(\"Took %f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码的运行速度会变慢，因为总共调用了 500 次的切片操作，这就是过度使用。一个更好的做法是采用 `torch.unbind` 运算符在每次循环中将矩阵切片为一个向量的列表，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.003989 seconds.\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([500, 10])\n",
    "z = torch.zeros([10])\n",
    "\n",
    "start = time.time()\n",
    "for x_i in torch.unbind(x):\n",
    "    z += x_i\n",
    "print(\"Took %f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文作者认为`torch.unbind`改进会提高一些速度。但从上文结果看出优化效果并不大，正确的做法应该是采用 `torch.sum` 来一步实现累加的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.000998 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "z = torch.sum(x, dim=0)\n",
    "print(\"Took %f seconds.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用`torch.sum()`实现累加速度就非常快了啊！\n",
    "\n",
    "其他重载的算数和逻辑运算符分别是：\n",
    "```python\n",
    "z = -x  # z = torch.neg(x)\n",
    "z = x + y  # z = torch.add(x, y)\n",
    "z = x - y\n",
    "z = x * y  # z = torch.mul(x, y)\n",
    "z = x / y  # z = torch.div(x, y)\n",
    "z = x // y\n",
    "z = x % y\n",
    "z = x ** y  # z = torch.pow(x, y)\n",
    "z = x @ y  # z = torch.matmul(x, y)\n",
    "z = x > y\n",
    "z = x >= y\n",
    "z = x < y\n",
    "z = x <= y\n",
    "z = abs(x)  # z = torch.abs(x)\n",
    "z = x & y\n",
    "z = x | y\n",
    "z = x ^ y  # z = torch.logical_xor(x, y)\n",
    "z = ~x  # z = torch.logical_not(x)\n",
    "z = x == y  # z = torch.eq(x, y)\n",
    "z = x != y  # z = torch.ne(x, y)\n",
    "```\n",
    "可以使用这些运算符的递增版本，比如 `x += y` 和 `x **=2` 都是合法的。另外，Python 并不允许重载 `and` 、`or` 和 `not` 三个关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch TorchScript 运行时间优化\n",
    "PyTorch 优化了高维度的张量的运算。在 PyTorch 中对小张量进行太多的运算操作是非常低效的。所以有可能的话，将计算操作都重写为批次（batch）的形式，可以减少消耗和提高性能。而如果没办法自己手动实现批次的运算操作，那么可以采用 TorchScript 来提升代码的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchScript 是一个 Python 函数的子集，但经过了 PyTorch 的验证，PyTorch 可以通过其 **just in time(jit)** 编译器来自动优化 TorchScript 代码，提高性能。\n",
    "\n",
    "以具体例子说明。在机器学习应用中非常常见的操作就是 `batch gather` ，也就是 `output[i] = input[i, index[i]]`可以理解为收集特定位置的值。其代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def batch_gather(tensor, indices):\n",
    "    output = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        output += [tensor[i][indices[i]]]\n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 `torch.jit.script` 装饰器来使用 TorchScript 的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def batch_gather_jit(tensor, indices):\n",
    "    output = []\n",
    "    for i in range(tensor.size(0)):\n",
    "        output += [tensor[i][indices[i]]]\n",
    "    return torch.stack(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个做法可以提高 10% 的运算速度(未验证)。 但更好的做法还是手动实现批次的运算操作，下面是一个向量化实现的代码例子，提高了 100 倍的速度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gather_vec(tensor, indices):\n",
    "    shape = list(tensor.shape)\n",
    "    flat_first = torch.reshape(\n",
    "        tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "    offset = torch.reshape(\n",
    "        torch.arange(shape[0]).cuda() * shape[1],\n",
    "        [shape[0]] + [1] * (len(indices.shape) - 1))\n",
    "    output = flat_first[indices + offset]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PS：这部分其实可以直接调用 `torch.gather()` 函数实现效果**，参考另一篇文章[PyTorch 函数解释：torch.gather()](https://blog.csdn.net/DreamHome_S/article/details/106032920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 构建高效 dataloader 类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如需代码运行更快，可以考虑如何将数据更加高效加载到内存中。PyTorch 提供了一个很容易加载数据的工具，即 `DataLoader` 。`DataLoader` 会采用多个 `workers` 来同时将数据从 `Dataset` 类中加载，并且可以选择使用 `Sampler` 类来对采样数据和组成 `batch` 形式的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你可以随时访问你的数据，那么使用 DataLoader 会非常简单：只需要继承 `Dataset` 类别并实现 `__getitem__` (读取每个数据)和 `__len__`（返回数据集的样本数量）这两个方法。\n",
    "\n",
    "下面给出一个代码例子，如何从给定的文件夹中加载图片数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class ImageDirectoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(path, pattern):\n",
    "        self.paths = list(glob.glob(os.path.join(path, pattern)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __item__(self):\n",
    "        path = random.choice(paths)\n",
    "        return cv2.imread(path, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如想将文件夹内所有的 jpeg 图片都加载，代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(ImageDirectoryDataset(\"/data/imagenet/*.jpg\"), num_workers=8)\n",
    "for data in dataloader:\n",
    "    # do something with data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里采用了 8 个 `workers` 来并行的从硬盘中读取数据。这个数量可以根据实际使用机器来进行调试，得到一个最佳的数量。\n",
    "\n",
    "当你的数据都很大或者你的硬盘读写速度很快，采用DataLoader进行随机读取数据是可行的。但也可能存在一种情况，就是使用的是一个很慢的连接速度的网络文件系统，请求单个文件的速度都非常的慢，而这可能就是整个训练过程中的瓶颈。\n",
    "\n",
    "一个更好的做法就是将数据保存为一个可以连续读取的连续文件格式。例如，当你有非常大量的图片数据，可以采用 tar 命令将其压缩为一个文件，然后用 python 来从这个压缩文件中连续的读取图片。要实现这个操作，需要用到 PyTorch 的 `IterableDataset`。创建一个 IterableDataset 类，只需要实现 `__iter__` 方法即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import torch\n",
    "\n",
    "def tar_image_iterator(path):\n",
    "    tar = tarfile.open(self.path, \"r\")\n",
    "    for tar_info in tar:\n",
    "        file = tar.extractfile(tar_info)\n",
    "        content = file.read()\n",
    "        yield cv2.imdecode(content, 1)\n",
    "        file.close()\n",
    "        tar.members = []\n",
    "    tar.close()\n",
    "\n",
    "class TarImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, path):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from tar_image_iterator(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此方法有一个问题，当使用 DataLoader 以及多个 workers 读取这个数据集的时候，会得到很多重复的数据。这个问题主要是因为每个 `worker` 都会创建一个单独的数据集的实例，并且都是从数据集的起始位置开始读取数据。一种避免这个问题的办法就是不是压缩为一个 tar 文件，而是将数据划分成 num_workers 个单独的 tar 文件，然后每个 worker 分别加载一个，代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, paths):\n",
    "        super().__init__()\n",
    "        self.paths = paths\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        # For simplicity we assume num_workers is equal to number of tar files\n",
    "        if worker_info is None or worker_info.num_workers != len(self.paths):\n",
    "            raise ValueError(\"Number of workers doesn't match number of files.\")\n",
    "        yield from tar_image_iterator(self.paths[worker_info.worker_id])\n",
    "        \n",
    "        \n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    TarImageDataset([\"/data/imagenet_part1.tar\", \"/data/imagenet_part2.tar\"]), num_workers=2)\n",
    "for data in dataloader:\n",
    "    # do something with data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 数值稳定性\n",
    "在我们使用任意一个数值计算库时，比如 NumPy 或者 PyTorch ，都需要知道一点，编写数学上正确的代码不一定会得到正确的结果，你需要确保这个计算是稳定的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以实例说明。从数学上来说，对任意的非零 $x$ ，可知式子 $x*y/y=x$ 是成立的。 再看具体实现时候的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevTools\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.float32(1)\n",
    "\n",
    "y = np.float32(1e-50)  # y would be stored as zero\n",
    "z = x * y / y\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码的运行结果是打印 `nan` ，原因是 y 的数值对于 float32 类型来说非常的小，这导致它的实际数值是 0 而不是 1e-50。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevTools\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.float32(1e39)  # y would be stored as inf\n",
    "z = x * y / y\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于$y$非常大的情况输出结果依然是 nan ，因为 y 太大而被存储为 inf 的情况，对于 float32 类型来说，其范围是 `1.4013e-45 ~ 3.40282e+38`，当超过这个范围，就会被置为 0 或者 inf。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何查看一种数据类型的数值范围："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-45\n",
      "3.4028235e+38\n",
      "-3.4028235e+38\n"
     ]
    }
   ],
   "source": [
    "print(np.nextafter(np.float32(0), np.float32(1)))  # prints 1.4013e-45\n",
    "print(np.finfo(np.float32).max)  # print 3.40282e+38\n",
    "print(np.finfo(np.float32).min)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让计算变得稳定，需要避免过大或者过小的数值。这看起来很容易，但这类问题是很难进行调试，特别是在 PyTorch 中进行梯度下降的时候。这不仅因为需要确保在前向传播过程中的所有数值都在使用的数据类型的取值范围内，还要保证在反向传播中也做到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出一个代码例子，计算一个输出向量的 softmax，一种不好的代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan  0.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def unstable_softmax(logits):\n",
    "    exp = torch.exp(logits)\n",
    "    return exp / torch.sum(exp)\n",
    "\n",
    "print(unstable_softmax(torch.tensor([1000., 0.])).numpy())  # prints [ nan, 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 logits 的指数数值可能会得到超出 float32 类型的取值范围，即过大或过小的数值，这里最大的 logits 数值是 `ln(3.40282e+38) = 88.7`，超过这个数值都会导致 nan 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么应该如何避免这种情况，做法很简单。因为有 $\\frac{\\exp (x-c)}{\\sum \\exp (x-c)}=\\frac{\\exp (x)}{\\sum \\exp (x)}$，也就是我们可以对 logits 减去一个常量，但结果保持不变，所以我们选择 logits 的最大值作为这个常数，这种做法，指数函数的取值范围就会限制为 [-inf, 0] ，然后最终的结果就是 [0.0, 1.0] 的范围，代码实现如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(logits):\n",
    "    exp = torch.exp(logits - torch.max(logits))\n",
    "    return exp / torch.sum(exp)\n",
    "\n",
    "print(softmax(torch.tensor([1000., 0.])).numpy())  # prints [ 1., 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设现在有一个分类问题。采用 softmax 函数对输出值 logits 计算概率。接着定义采用预测值和标签的交叉熵作为损失函数。对于一个类别分布的交叉熵可以简单定义为 ：\n",
    "$$x e(p, q)=-\\sum p_{i} \\log \\left(q_{i}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "# 不稳定地实现\n",
    "def unstable_softmax_cross_entropy(labels, logits):\n",
    "    logits = torch.log(softmax(logits))\n",
    "    return -torch.sum(labels * logits)\n",
    "\n",
    "labels = torch.tensor([0.5, 0.5])\n",
    "logits = torch.tensor([1000., 0.])\n",
    "\n",
    "xe = unstable_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "print(xe.numpy())  # prints inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码实现中，当 softmax 结果趋向于 0，其 log 输出会趋向于无穷，这就导致计算结果的不稳定性。所以可以对其进行重写，将 softmax 维度拓展并做一些归一化的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "def softmax_cross_entropy(labels, logits, dim=-1):\n",
    "    scaled_logits = logits - torch.max(logits)\n",
    "    normalized_logits = scaled_logits - torch.logsumexp(scaled_logits, dim)\n",
    "    return -torch.sum(labels * normalized_logits)\n",
    "\n",
    "labels = torch.tensor([0.5, 0.5])\n",
    "logits = torch.tensor([1000., 0.])\n",
    "\n",
    "xe = softmax_cross_entropy(labels, logits)\n",
    "\n",
    "print(xe.numpy())  # prints 500.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以验证计算的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5 -0.5]\n"
     ]
    }
   ],
   "source": [
    "logits.requires_grad_(True)\n",
    "xe = softmax_cross_entropy(labels, logits)\n",
    "g = torch.autograd.grad(xe, logits)[0]\n",
    "print(g.numpy())  # prints [0.5, -0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次提醒，进行梯度下降操作的时候需要额外的小心谨慎，需要确保每个网络层的函数和梯度的范围都在合法的范围内，指数函数和对数函数在不正确使用的时候都可能导致很大的问题，它们都能将非常小的数值转换为非常大的数值，或者从很大变为很小的数值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
